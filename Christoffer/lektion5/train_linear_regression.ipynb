{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWMAL Exercise\n",
    "\n",
    "(In the following you need not present your journal in the Qa+b+c+ etc. order. You could just present the final code with test and comments.)\n",
    "\n",
    "## Training Your Own Linear Regressesor\n",
    "\n",
    "Create a linear regressor, with a Scikit-learn compatible fit-predict interface. You should implement every detail of the linear regressor in Python, using whatever library you want (except a linear regressor itself).\n",
    "\n",
    "You must investigate and describe all major details for a linear regressor, and implement at least the following concepts (MUST):\n",
    "\n",
    "### Qa: Concepts and Implementations MUSTS\n",
    "\n",
    "* the `fit-predict` interface, and a $R^2$ score function,\n",
    "* one-dimensional output only,\n",
    "* loss function based on (R)MSE,\n",
    "* setting of the number of iterations and learning rate ($\\eta$) via parameters in the constructor, the signature of your `__init__` must include the named parameters `max_iter` and `eta0`,\n",
    "* the batch-gradient decent algorithm (GD),\n",
    "* constant or adaptive learning rate,\n",
    "* learning graphs,\n",
    "* stochastic gradient descent (SGD),\n",
    "* epochs vs iteations,\n",
    "* compare the numerical optimization with the Closed-form solution.\n",
    "\n",
    "### Qb: [OPTIONAL] Additional Concepts and Implementations\n",
    "\n",
    "And perhaps you could include (SHOULD/COULD):\n",
    "\n",
    "* (stochastic) mini-bach gradient decent, \n",
    "* early stopping,\n",
    "* interface to your bias and weights via `intercept_` and `coef_` attributes on your linear regressor `class`,\n",
    "* get/set functionality of your regressor, such that is fully compatible with other Scikit-learn algorithms, try it out in say a `cross_val_score()` call from Scikit-learn,\n",
    "* test in via the smoke tests at the end of this Notebook,\n",
    "* testing it on MNIST data, \n",
    "\n",
    "With the following no-nos (WONT):\n",
    "\n",
    "* no multi-linear regression,\n",
    "* no reuse of the Scikit-learn regressor\n",
    "* no `C/C++` optimized implementaion with a _thin_ Python interface (nifty, but to much work for now),\n",
    "* no copy-paste of code from other sources WITHOUT a clear cite/reference for your source.\n",
    "\n",
    "### Qc: Testing and Test Data\n",
    "\n",
    "Use mainly very low-dimensional data for testing, say the Iris data, since it might be very slow. Or create a simple low-dimensionality data generator.\n",
    "\n",
    "If you are brave, try your regressor on MNIST, and be prepared to tune on your learning rate, $\\eta$, since your regressor can behave badly (loss becomes higher and higher for each iteration).\n",
    "\n",
    "### Qd: The Journaling of Your Regressor \n",
    "\n",
    "For the journal, write a full explanation of how you implemented the linear regressor, including a code walk-through (or mini-review of the most interesting parts).\n",
    "\n",
    "### Qe: Mathematical Foundation for Training a Linear Regressor\n",
    "\n",
    "You must also include the theoretical mathematical foundation for the linear regressor using the following equations and graphs (free to include in your journal without cite/reference), and relate them directly to your code:\n",
    "\n",
    "* Design matrix of size $(n, d)$ where each row is an input column vector $(\\mathbf{x}^{(i)})^\\top$ data sample of size $d$\n",
    "$$  \\def\\rem#1{}\n",
    "    \\rem{ITMAL: CEF def and LaTeX commands, remember: no  newlines in defs}\n",
    "    \\def\\eq#1#2{#1 &=& #2\\\\}\n",
    "    \\def\\ar#1#2{\\begin{array}{#1}#2\\end{array}}\n",
    "    \\def\\ac#1#2{\\left[\\ar{#1}{#2}\\right]}\n",
    "    \\def\\st#1{_{\\textrm{#1}}}\n",
    "    \\def\\norm#1{{\\cal L}_{#1}}\n",
    "    \\def\\obs#1#2{#1_{\\textrm{\\scriptsize obs}}^{\\left(#2\\right)}}\n",
    "    \\def\\diff#1{\\mathrm{d}#1}\n",
    "    \\def\\pown#1{^{(#1)}}\n",
    "    \\def\\pownn{\\pown{n}}\n",
    "    \\def\\powni{\\pown{i}}\n",
    "    \\def\\powtest{\\pown{\\textrm{\\scriptsize test}}}\n",
    "    \\def\\powtrain{\\pown{\\textrm{\\scriptsize train}}}\n",
    "    \\def\\bX{\\mathbf{M}}\n",
    "    \\def\\bX{\\mathbf{X}}\n",
    "    \\def\\bZ{\\mathbf{Z}}\n",
    "    \\def\\bw{\\mathbf{m}}\n",
    "    \\def\\bx{\\mathbf{x}}\n",
    "    \\def\\by{\\mathbf{y}}\n",
    "    \\def\\byt{\\mathbf{y}\\st{true}}\n",
    "    \\def\\yt{y\\st{true}}\n",
    "    \\def\\bz{\\mathbf{z}}\n",
    "    \\def\\bw{\\mathbf{w}}\n",
    "    \\def\\btheta{{\\boldsymbol\\theta}}\n",
    "    \\def\\bSigma{{\\boldsymbol\\Sigma}}\n",
    "    \\def\\half{\\frac{1}{2}}\n",
    "    \\def\\pfrac#1#2{\\frac{\\partial~#1}{\\partial~#2}}\n",
    "    \\def\\dfrac#1#2{\\frac{\\mathrm{d}~#1}{\\mathrm{d}#2}}\n",
    "\\bX =\n",
    "        \\ac{cccc}{\n",
    "            x_1\\pown{1} & x_2\\pown{1} & \\cdots & x_d\\pown{1} \\\\\n",
    "            x_1\\pown{2} & x_2\\pown{2} & \\cdots & x_d\\pown{2}\\\\\n",
    "            \\vdots      &             &        & \\vdots \\\\\n",
    "            x_1\\pownn   & x_2\\pownn   & \\cdots & x_d\\pownn\\\\\n",
    "        } \n",
    "$$ \n",
    "\n",
    "* Target ground-truth column vector of size $n$\n",
    "\n",
    "$$\n",
    "\\byt =\n",
    "  \\ac{c}{\n",
    "     \\yt\\pown{1} \\\\\n",
    "     \\yt\\pown{2} \\\\\n",
    "     \\vdots \\\\\n",
    "     \\yt\\pown{n} \\\\\n",
    "  } \n",
    "$$\n",
    "\n",
    "* Bias factor, and by convention in the following (appended one)\n",
    "\n",
    "$$\n",
    "\\ar{rl}{\n",
    "  \\ac{c}{1\\\\\\bx\\powni} &\\mapsto \\bx\\powni\\\\\n",
    "}\n",
    "$$\n",
    "\n",
    "* Linear regression model hypothesis function for a column vector input $\\bx\\powni$ of size $d$ and a column weight vector $\\bw$ of size $d+1$ (with the additional element $w_0$ being the bias)\n",
    "\n",
    "$$\n",
    "\\ar{rll}{\n",
    "  ~~~~~~~~~~~~~~~\n",
    "  h(\\bx\\powni;\\bw) &= y\\st{pred}\\powni \\\\\n",
    "                   &= \\bw^\\top \\bx\\powni  & (\\bx\\powni~\\textrm{with bias})\\\\ \n",
    "                   &= w_0  \\cdot 1+ w_1 x_1\\powni + w_2 x_2\\powni + \\cdots + w_d x_d\\powni & \\\\\n",
    "}\n",
    "$$\n",
    "\n",
    "* Individual losses based on the $\\norm{2}^2$ (last part asuming one dimenesional output)\n",
    "\n",
    "$$\n",
    "\\ar{rl}{\n",
    "  L\\powni &= || y\\st{pred} \\powni     - y\\st{true}\\powni~ ||_2^2\\\\\n",
    "          &= || h(\\bx\\powni;\\bw)      - y\\st{true}\\powni~ ||_2^2\\\\\n",
    "          &= || \\bw^\\top\\bx\\powni     - y\\st{true}\\powni~ ||_2^2\\\\\n",
    "          &= \\left( \\bw^\\top\\bx\\powni - y\\st{true}\\powni~\\right)^2\n",
    "}\n",
    "$$\n",
    "\n",
    "* MSE loss function\n",
    "\n",
    "$$\n",
    "  \\ar{rl}{\n",
    "  \\textrm{MSE}(\\bX,\\by;\\bw) &= \\frac{1}{n} \\sum_{i=1}^{n} L\\powni \\\\\n",
    "                    &= \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\bw^\\top\\bx\\powni - y\\st{true}\\powni \\right)^2\\\\\n",
    "                    &= \\frac{1}{n} ||\\bX \\bw - \\byt~||_2^2\n",
    "}\n",
    "$$                   \n",
    "\n",
    "* Loss function, proportional to (R)MSE\n",
    "\n",
    "$$\n",
    "  \\ar{rl}{\n",
    "     J &= \\frac{1}{2} ||\\bX \\bw - \\byt~||_2^2\\\\\n",
    "       &\\propto \\textrm{MSE}\n",
    "}\n",
    "$$\n",
    "\n",
    "* Training: computing the optimal value of the $\\bw$ weight; that is finding the $\\bw$-value that minimizes the total loss\n",
    "\n",
    "$$\n",
    "  \\bw^* = \\textrm{argmin}_\\bw~J\\\\\n",
    "$$\n",
    "\n",
    "* Visualization of $\\textrm{argmin}_\\bw$ means to the argument of $\\bw$ that minimizes the $J$ function. The minimization can in 2-D visually be drawn as finding the lowest $J$ that for linear regression always forms a convex shape \n",
    "\n",
    "<img src=\"https://itundervisning.ase.au.dk/SWMAL/L05/Figs/minimization.png\" alt=\"WARNING: could not get image from the server.\" style=\"height:240px\">\n",
    "\n",
    "### Traning I: The Closed-form Solution\n",
    "\n",
    "* Finding the optimal weight in a _on-step_ analytic expression \n",
    "\n",
    "$$\n",
    "  \\bw^* ~=~ \\left( \\bX^\\top \\bX \\right)^{-1}~ \\bX^\\top \\byt\n",
    "$$\n",
    "\n",
    "\n",
    "### Training II: Numerical Optimization \n",
    "\n",
    "* The Gradient of the loss function\n",
    "\n",
    "$$   \n",
    "  \\nabla_\\bw~J = \\left[ \\frac{\\partial J}{\\partial w_1} ~~~~ \\frac{\\partial J}{\\partial w_2} ~~~~ \\ldots  ~~~~ \\frac{\\partial J}{\\partial w_d} \\right]^\\top\n",
    "$$\n",
    "\n",
    "* The Gradient for the based $J$\n",
    "\n",
    "$$\n",
    "\\ar{rl}{\n",
    "  \\nabla_\\bw J &= \\frac{2}{n} \\bX^\\top \\left( \\bX \\bw - \\byt \\right)\n",
    "}\n",
    "$$\n",
    "\n",
    "* The Gradient Decent Algorithm (GD)\n",
    "\n",
    "$$ \n",
    "  \\bw^{(step~N+1)}~ = \\bw^{(step~N)} - \\eta \\nabla_{\\bw} J\n",
    "$$\n",
    "\n",
    "* Visualization of GD in 2-D\n",
    "\n",
    "<img src=\"https://itundervisning.ase.au.dk/SWMAL/L05/Figs/minimization_gd.png\" alt=\"WARNING: could not get image from the server.\" style=\"height:240px\">"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class MyLinReg:\n",
    "    def __init__(self, max_iter=1000, eta0=0.01):\n",
    "        self.max_iter = max_iter\n",
    "        self.eta0 = eta0\n",
    "        self.intercept_ = None\n",
    "        self.coef_ = None\n",
    "\n",
    "    def _initialize_weights(self, n_features):\n",
    "        self.intercept_ = 0.0\n",
    "        self.coef_ = np.zeros(n_features)\n",
    "\n",
    "    def _compute_loss(self, X, y):\n",
    "        n_samples = X.shape[0]\n",
    "        predictions = self.intercept_ + np.dot(X, self.coef_)\n",
    "        loss = np.sqrt(np.mean((predictions - y) ** 2))\n",
    "        return loss\n",
    "\n",
    "    def _compute_gradients(self, X, y):\n",
    "        n_samples = X.shape[0]\n",
    "        predictions = self.intercept_ + np.dot(X, self.coef_)\n",
    "        error = predictions - y\n",
    "        grad_intercept = np.mean(error)\n",
    "        grad_coef = np.dot(X.T, error) / n_samples\n",
    "        return grad_intercept, grad_coef\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self._initialize_weights(X.shape[1])\n",
    "        for i in range(self.max_iter):\n",
    "            grad_intercept, grad_coef = self._compute_gradients(X, y)\n",
    "            self.intercept_ -= self.eta0 * grad_intercept\n",
    "            self.coef_ -= self.eta0 * grad_coef\n",
    "\n",
    "            # Optional: Implement learning rate schedule for adaptive eta0\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.intercept_ + np.dot(X, self.coef_)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        predictions = self.predict(X)\n",
    "        u = ((y - predictions) ** 2).sum()\n",
    "        v = ((y - y.mean()) ** 2).sum()\n",
    "        return 1 - u / v\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T14:09:15.045027Z",
     "start_time": "2024-02-27T14:09:14.758375Z"
    }
   },
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qf: Smoke testing\n",
    "\n",
    "Once ready, you can test your regressor via the test stub below, or create your own _test-suite_.\n",
    "\n",
    "Be aware that setting the stepsize, $\\eta$, value can be tricky, and you might want to tune `eta0` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T14:13:27.035403Z",
     "start_time": "2024-02-27T14:13:27.018242Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred = [5.61498304 6.75547481 4.04730809 5.18372265]\n",
      "SCORE = 0.49500564295554395\n",
      "bias         = 4.046878010107266\n",
      "coefficients = [1.88012265]\n",
      "\tw         =[4.046878010107266, array([1.880122650194])]\n",
      "\tw_expected=[4.046879011698 1.880121487278]\n",
      "OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: could not import PrintMatrix from libitmal.utils, defaulting to primitive print..\n"
     ]
    }
   ],
   "source": [
    "# Mini smoke test for your linear regressor...\n",
    "\n",
    "import sys\n",
    "import numpy\n",
    "\n",
    "def Warn(msg, ex=None, pre_msg=\"WARNING\"):\n",
    "    if ex is not None:\n",
    "        msg += f\"\\n   EXCEPTION: {ex} ({type(ex)})\"\n",
    "    print(f\"{pre_msg}: {msg}\", file=sys.stderr)\n",
    "\n",
    "def Err(msg, ex=None):\n",
    "    Warn(msg, ex, \"ERROR\")\n",
    "    exit(-1)\n",
    "\n",
    "def PrintMatrix(x, label=\"\", precision=12):\n",
    "    # default simple implementation, may be overwritten by a libitmal function later..\n",
    "    print(f\"{label}{' ' if len(label)>0 else ''}{x}\")\n",
    "\n",
    "def GenerateData():\n",
    "    X = numpy.array([[8.34044009e-01],[1.44064899e+00],[2.28749635e-04],[6.04665145e-01]])\n",
    "    y = numpy.array([5.97396028, 7.24897834, 4.86609388, 3.51245674])\n",
    "    return X, y\n",
    "\n",
    "def TestMyLinReg():\n",
    "    X, y = GenerateData()\n",
    "    \n",
    "    try:\n",
    "        # assume that your regressor class is named 'MyLinReg', please update/change\n",
    "        regressor = MyLinReg()\n",
    "    except Exception as ex:\n",
    "        Err(\"your regressor has another name, than 'MyLinReg', please change the name in this smoke test\", ex)\n",
    "   \n",
    "    try:\n",
    "        regressor = MyLinReg(max_iter=200, eta0=0.4)\n",
    "    except Exception as ex:\n",
    "        Err(\"your regressor can not be constructed via the __init_ (with two parameters, see call above\", ex)\n",
    "\n",
    "    try:\n",
    "        regressor.fit(X, y)\n",
    "    except Exception as ex:\n",
    "        Err(\"your regressor can not fit\", ex)\n",
    "\n",
    "    try:\n",
    "        y_pred = regressor.predict(X)\n",
    "        print(f\"y_pred = {y_pred}\")\n",
    "    except Exception as ex:\n",
    "        Err(\"your regressor can not predict\", ex)\n",
    "\n",
    "    try:\n",
    "        score  = regressor.score(X, y)\n",
    "        print(f\"SCORE = {score}\")\n",
    "    except Exception as ex:\n",
    "        Err(\"your regressor fails in the score call\", ex)\n",
    "\n",
    "    try:\n",
    "        w    = None # default\n",
    "        bias = None # default\n",
    "        try:\n",
    "            w = regressor.coef_\n",
    "            bias = regressor.intercept_\n",
    "        except Exception as ex:\n",
    "            w = None\n",
    "            Warn(\"your regressor has no coef_/intercept_ atrributes, trying Weights() instead..\", ex)\n",
    "        try:\n",
    "            if w is None:\n",
    "                w = regressor.Weights() # maybe a Weigths function is avalible on you model? \n",
    "                try:\n",
    "                    assert w.ndim == 1,     \"can only handle vector like w's for now\"\n",
    "                    assert w.shape[0] >= 2, \"expected length of to be at least 2, that is one bias one coefficient\" \n",
    "                    bias = w[0]\n",
    "                    w = w[1:]\n",
    "                except Exception as ex:\n",
    "                    w = None\n",
    "                    Err(\"having a hard time concantenating our bias and coefficients, giving up!\", ex) \n",
    "        except Exception as ex:\n",
    "            w = None\n",
    "            Err(\"your regressor also has no Weights() function, giving up!\", ex)\n",
    "        print(f\"bias         = {bias}\") \n",
    "        print(f\"coefficients = {w}\")    \n",
    "    except Exception as ex:\n",
    "        Err(\"your regressor fails during extraction of bias and weights (but is a COULD)\", ex)\n",
    "\n",
    "    try:\n",
    "        import sys,os\n",
    "        sys.path.append(R'../../UndervisningsGit/GITMAL')\n",
    "        from libitmal.utils import PrintMatrix, AssertInRange\n",
    "    except Exception as ex:\n",
    "        Warn(\"could not import PrintMatrix from libitmal.utils, defaulting to primitive print..\")\n",
    "\n",
    "    try:\n",
    "        if w is not None:\n",
    "            if bias is not None:\n",
    "                w = [bias, w[:]] # reconcat bias an coefficients, may be incorrect for your implementation!\n",
    "            # TEST VECTOR:\n",
    "            w_expected = numpy.array([4.046879011698, 1.880121487278])\n",
    "            PrintMatrix(w,          label=\"\\tw         =\", precision=12)\n",
    "            PrintMatrix(w_expected, label=\"\\tw_expected=\", precision=12)\n",
    "            AssertInRange(w, w_expected, eps=1E-5)\n",
    "        else:\n",
    "            Warn(\"cannot test due to missing w information\")\n",
    "    except Exception as ex:\n",
    "        Err(\"mini-smoketest on your regressor failed\", ex)\n",
    "\n",
    "TestMyLinReg()\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qg: [OPTIONAL] More Smoke-testing\n",
    "\n",
    "Or perhaps do a comparison test with the SGD regressor in Scikit-learn using the next smoke-test function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T14:14:14.197904Z",
     "start_time": "2024-02-27T14:13:53.144715Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA: 'IRIS'\n",
      "\tSHAPES: X_train=(112, 4), X_test=(38, 4), y_train=(112,), y_test=(38,)\n",
      "\n",
      "TRAINING['MyLinReg']..\n",
      "y_pred_test = [ 0.81500715  1.07770362 -0.33170291 -0.35008098  1.41982387  1.32158714\n",
      " -0.26294153  0.66825246 -0.51907273  1.29340223  1.23074812  1.11482336\n",
      " -0.37814141  0.43383765  1.17522155  0.60725387 -0.16637206 -0.36467974\n",
      " -0.45415098  0.72683954 -0.28602188 -0.41414007  1.44101574  1.07034312\n",
      " -0.35824606  0.86228891 -0.21549042  0.79488213 -0.36839488  0.61728431\n",
      " -0.34191589  0.74636416  0.74342204 -0.23571828  1.72983603 -0.39202139\n",
      " -0.40436807  1.52706895]\n",
      "SCORE['MyLinReg'] = 0.690\n",
      "\n",
      "TRAINING['SGDRegressor']..\n",
      "y_pred_test = [ 1.10023881e+00  1.58608966e+00 -3.31917080e-03 -5.11074074e-02\n",
      "  1.88293555e+00  1.84829866e+00  6.39167491e-02  1.02792964e+00\n",
      " -2.20429418e-01  1.83512080e+00  1.63586217e+00  1.52801398e+00\n",
      " -1.45655829e-01  8.36856282e-01  1.57443122e+00  1.00186623e+00\n",
      "  1.59333530e-01 -2.55771755e-02 -1.02304509e-01  1.11638373e+00\n",
      "  1.97913785e-03 -1.04193953e-01  1.80542398e+00  1.48200802e+00\n",
      " -2.93707946e-02  1.20750023e+00  1.42015084e-01  1.13732432e+00\n",
      " -5.60587488e-02  9.61905027e-01 -7.28440203e-02  1.09965204e+00\n",
      "  1.13927806e+00  4.55206826e-02  2.07454208e+00 -7.38302677e-02\n",
      " -6.25029971e-02  1.86076060e+00]\n",
      "SCORE['SGDRegressor'] = 0.947\n",
      "\n",
      "##############################################\n",
      "DATA: 'MNIST'\n",
      "\tSHAPES: X_train=(52500, 784), X_test=(17500, 784), y_train=(52500,), y_test=(17500,)\n",
      "\n",
      "TRAINING['MyLinReg']..\n",
      "y_pred_test = [3.84023043 0.4374833  0.89974107 ... 2.4964589  0.59880822 3.3544992 ]\n",
      "SCORE['MyLinReg'] = 0.278\n",
      "\n",
      "TRAINING['SGDRegressor']..\n",
      "y_pred_test = [ 1.00224942e+10 -8.09647357e+09  1.65221913e+12 ... -4.26064438e+10\n",
      " -1.19287068e+10  6.72002421e+08]\n",
      "SCORE['SGDRegressor'] = -28166944092489078800384.000\n",
      "\n",
      "##############################################\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model    import SGDRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing   import StandardScaler\n",
    "from sklearn.pipeline        import Pipeline\n",
    "\n",
    "from libitmal import dataloaders\n",
    "\n",
    "def TestAndCompareRegressors():\n",
    "    for f in [(\"IRIS\", dataloaders.IRIS_GetDataSet), (\"MNIST\", dataloaders.MNIST_GetDataSet)]:\n",
    "        data = f[1]() # returns (X, y)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(data[0], data[1])\n",
    "        \n",
    "        print(f\"DATA: '{f[0]}'\\n\\tSHAPES: X_train={X_train.shape}, X_test={X_test.shape}, y_train={y_train.shape}, y_test={y_test.shape}\")\n",
    "\n",
    "        regressor0 = MyLinReg(max_iter=1000, eta0=1E-3)\n",
    "        regressor1 = SGDRegressor()\n",
    "\n",
    "        for r in [(\"MyLinReg\", regressor0), (\"SGDRegressor\", regressor1)]:\n",
    "            print(f\"\\nTRAINING['{r[0]}']..\")\n",
    "            pipe = Pipeline([('scaler', StandardScaler()), r])\n",
    "            pipe.fit(X_train, y_train)\n",
    "            y_pred_test = pipe.predict(X_test)\n",
    "            print(f\"y_pred_test = {y_pred_test}\")  \n",
    "            r2 = pipe.score(X_test, y_test)\n",
    "            print(f\"SCORE['{r[0]}'] = {r2:0.3f}\")\n",
    "            \n",
    "        print(\"\\n##############################################\\n\")\n",
    "\n",
    "# somewhat more verbose testing, you regressor will likely fail on MNIST \n",
    "# or at least be very, very slow...\n",
    "TestAndCompareRegressors()\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qh Conclusion\n",
    "\n",
    "As always, take some time to fine-tune your regressor, perhaps just some code-refactoring, cleaning out 'bad' code, and summarize all your findings\n",
    " above. \n",
    "\n",
    "In other words, write a conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REVISIONS||\n",
    ":- | :- |\n",
    "2022-12-22| CEF, initial draft. \n",
    "2023-02-26| CEF, first release."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "varInspector": {
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
