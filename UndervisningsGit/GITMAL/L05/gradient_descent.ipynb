{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWMAL Exercise\n",
    "\n",
    "\n",
    "## Gradient Descent Methods and Training\n",
    "\n",
    "Finding the optimal solution in one-step, via \n",
    "\n",
    "$$\n",
    "    \\def\\rem#1{}\n",
    "    \\rem{ITMAL: CEF def and LaTeX commands, remember: no  newlines in defs}\n",
    "    \\def\\eq#1#2{#1 &=& #2\\\\}\n",
    "    \\def\\ar#1#2{\\begin{array}{#1}#2\\end{array}}\n",
    "    \\def\\ac#1#2{\\left[\\ar{#1}{#2}\\right]}\n",
    "    \\def\\st#1{_{\\textrm{\\scriptsize #1}}}\n",
    "    \\def\\norm#1{{\\cal L}_{#1}}\n",
    "    \\def\\obs#1#2{#1_{\\textrm{\\scriptsize obs}}^{\\left(#2\\right)}}\n",
    "    \\def\\diff#1{\\mathrm{d}#1}\n",
    "    \\def\\pown#1{^{(#1)}}\n",
    "    \\def\\pownn{\\pown{n}}\n",
    "    \\def\\powni{\\pown{i}}\n",
    "    \\def\\powtest{\\pown{\\textrm{\\scriptsize test}}}\n",
    "    \\def\\powtrain{\\pown{\\textrm{\\scriptsize train}}}\n",
    "    \\def\\bX{\\mathbf{M}}\n",
    "    \\def\\bX{\\mathbf{X}}\n",
    "    \\def\\bZ{\\mathbf{Z}}\n",
    "    \\def\\bw{\\mathbf{m}}\n",
    "    \\def\\bx{\\mathbf{x}}\n",
    "    \\def\\by{\\mathbf{y}}\n",
    "    \\def\\bz{\\mathbf{z}}\n",
    "    \\def\\bw{\\mathbf{w}}\n",
    "    \\def\\btheta{{\\boldsymbol\\theta}}\n",
    "    \\def\\bSigma{{\\boldsymbol\\Sigma}}\n",
    "    \\def\\half{\\frac{1}{2}}\n",
    "    \\def\\pfrac#1#2{\\frac{\\partial~#1}{\\partial~#2}}\n",
    "    \\def\\dfrac#1#2{\\frac{\\mathrm{d}~#1}{\\mathrm{d}#2}}\n",
    "\\bw^* ~=~ \\left( \\bX^\\top \\bX \\right)^{-1} \\bX^\\top \\by\n",
    "$$\n",
    "\n",
    "has its downsides: the scaling problem of the matrix inverse. Now, let us look at a numerical solution to the problem of finding the value of $\\bw$  (aka $\\btheta$) that minimizes the objective function $J$.\n",
    "\n",
    "Again, ideally we just want to find places, where the (multi-dimensionally) gradient of $J$ is zero (here using a constant factor $\\frac{2}{m}$)\n",
    "\n",
    "$$\n",
    "\\ar{rl}{\n",
    "  \\nabla_\\bw J(\\bw) &= \\frac{2}{m} \\bX^\\top \\left( \\bX \\bw - \\by \\right)\\\\\n",
    "}\n",
    "$$\n",
    "\n",
    "and numerically we calculate $\\nabla_{\\bw} J$ for a point in $\\bw$-space, and then move along in the opposite direction of this gradient, taking a step of size $\\eta$\n",
    "\n",
    "$$ \n",
    "  \\bw^{(step~N+1)} = \\bw^{(step~N)} - \\eta \\nabla_{\\bw} J(\\bw)\n",
    "$$\n",
    "\n",
    "That's it, pretty simple, right (apart from numerical stability, problem with convergence and regularization, that we will discuss later).\n",
    "\n",
    "So, we begin with some initial $\\bw$, and iterate via the equation above, towards places, where $J$ is smaller, and this can be illustrated as\n",
    "\n",
    "<img src=\"https://itundervisning.ase.au.dk/SWMAL/L05/Figs/minimization_gd.png\" alt=\"WARNING: could not get image from server.\" style=\"height:240px\">\n",
    "\n",
    "\n",
    "If we hit the/a global minimum or just a local minimum (or in extremely rare cases a local saddle point) is another question when not using a simple linear regression model: for non-linear models we will in general not see a nice convex $J$-$\\bw$ surface, as in the figure above.\n",
    "\n",
    "### Qa The Gradient Descent Method (GD)\n",
    "\n",
    "Explain the gradient descent algorithm using the equations in the section _'ยง Bach Gradient Descent'_ [HOML, p.121-122 2nd, p.143 3rd], and relate it to the code snippet \n",
    "\n",
    "```python\n",
    "X_b, y = GenerateData()\n",
    "\n",
    "eta = 0.1\n",
    "n_iterations = 1000\n",
    "m = 100\n",
    "theta = np.random.randn(2,1)\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "    theta = theta - eta * gradients\n",
    "``` \n",
    "in the python code below. \n",
    "\n",
    "As usual, avoid going top much into details of the code that does the plotting.\n",
    "\n",
    "What role does `eta` play, and what happens if you increase/decrease it (explain the three plots)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Qa...examine the method (without the plotting)\n",
    "\n",
    "# NOTE: modified code from [GITHOML], 04_training_linear_models.ipynb\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def GenerateData():\n",
    "    X = 2 * np.random.rand(100, 1)\n",
    "    y = 4 + 3 * X + np.random.randn(100, 1)\n",
    "    X_b = np.c_[np.ones((100, 1)), X]  # add x0 = 1 to each instance\n",
    "    return X, X_b, y\n",
    "\n",
    "X, X_b, y = GenerateData()\n",
    "\n",
    "eta = 0.1\n",
    "n_iterations = 1000\n",
    "m = 100\n",
    "theta = np.random.randn(2,1)\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "    theta = theta - eta * gradients\n",
    "\n",
    "print(f'stochastic gradient descent theta={theta.ravel()}')\n",
    "\n",
    "##########################################################\n",
    "# rest of the code is just for plotting, needs no review\n",
    "\n",
    "def plot_gradient_descent(theta, eta, theta_path=None):\n",
    "    m = len(X_b)\n",
    "    plt.plot(X, y, \"b.\")\n",
    "    n_iterations = 1000\n",
    "    for iteration in range(n_iterations):\n",
    "        if iteration < 10:\n",
    "            y_predict = X_new_b.dot(theta)\n",
    "            style = \"b-\" if iteration > 0 else \"r--\"\n",
    "            plt.plot(X_new, y_predict, style)\n",
    "        \n",
    "        gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "        theta = theta - eta * gradients\n",
    "        if theta_path is not None:\n",
    "            theta_path.append(theta)\n",
    "    plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "    plt.axis([0, 2, 0, 15])\n",
    "    plt.title(r\"$\\eta = {}$\".format(eta), fontsize=16)\n",
    "\n",
    "np.random.seed(42)\n",
    "theta_path_bgd = []\n",
    "theta = np.random.randn(2,1)  # random initialization\n",
    "\n",
    "X_new = np.array([[0], [2]])\n",
    "X_new_b = np.c_[np.ones((2, 1)), X_new]  # add x0 = 1 to each instance\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(131); plot_gradient_descent(theta, eta=0.02)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.subplot(132); plot_gradient_descent(theta, eta=0.1, theta_path=theta_path_bgd)\n",
    "plt.subplot(133); plot_gradient_descent(theta, eta=0.5)\n",
    "plt.show()\n",
    "\n",
    "print('OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qb The Stochastic Gradient Descent Method (SGD)\n",
    "\n",
    "Now, introducing the _stochastic_ variant of gradient descent, explain the stochastic nature of the SGD, and comment on the difference to the _normal_ gradient descent method (GD) we just saw.\n",
    "\n",
    "Also explain the role of the calls to `np.random.randint()` in the code, \n",
    "\n",
    "HINT: In detail, the important differences are, that the main loop for SGC is \n",
    "```python\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        .\n",
    "        .\n",
    "        .\n",
    "        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n",
    "        eta = ...\n",
    "        theta = ...\n",
    "```\n",
    "where it for the GD method was just\n",
    "```python\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "    theta = ..\n",
    "```\n",
    "\n",
    "NOTE: the call `np.random.seed(42)` resets the random generator so that it produces the same random-sequence when re-running the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Qb...run this code\n",
    "\n",
    "# NOTE: code from [GITHOML], 04_training_linear_models.ipynb\n",
    "\n",
    "theta_path_sgd = []\n",
    "m = len(X_b)\n",
    "np.random.seed(42)\n",
    "\n",
    "n_epochs = 50\n",
    "t0, t1 = 5, 50  # learning schedule hyperparameters\n",
    "\n",
    "def learning_schedule(t):\n",
    "    return t0 / (t + t1)\n",
    "\n",
    "theta = np.random.randn(2,1)  # random initialization\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        if epoch == 0 and i < 20:\n",
    "            y_predict = X_new_b.dot(theta) \n",
    "            style = \"b-\" if i > 0 else \"r--\"\n",
    "            plt.plot(X_new, y_predict, style)\n",
    "        \n",
    "        random_index = np.random.randint(m)\n",
    "        xi = X_b[random_index:random_index+1]\n",
    "        yi = y[random_index:random_index+1]\n",
    "\n",
    "        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)        \n",
    "        eta = learning_schedule(epoch * m + i)\n",
    "        theta = theta - eta * gradients\n",
    "        theta_path_sgd.append(theta)                 \n",
    "\n",
    "        plt.plot(X, y, \"b.\")      \n",
    "\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "sgd_reg = SGDRegressor(max_iter=50, tol=-np.infty, penalty=None, eta0=0.1, random_state=42)\n",
    "sgd_reg.fit(X, y.ravel())\n",
    "print(f'stochastic gradient descent theta={theta.ravel()}')\n",
    "print(f'Scikit-learn SGDRegressor \"thetas\": sgd_reg.intercept_={sgd_reg.intercept_}, sgd_reg.coef_={sgd_reg.coef_}')\n",
    "\n",
    "##########################################################\n",
    "# rest of the code is just for plotting, needs no review \n",
    "plt.xlabel(\"$x_1$\", fontsize=18)                     \n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)           \n",
    "plt.axis([0, 2, 0, 15])                              \n",
    "\n",
    "plt.show()        \n",
    "\n",
    "print('OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qc Adaptive learning rate for $\\eta$  \n",
    "\n",
    "There is also an adaptive learning rate method in the demo code for the SGD. \n",
    "\n",
    "Explain the effects of the `learning_schedule()` functions.\n",
    "\n",
    "You can set the learning rate parameter (also known as a hyperparameter) in may ML algorithms, say for SGD regression, to a method of your choice \n",
    "\n",
    "```python\n",
    "SGDRegressor(max_iter=1,\n",
    "             eta0=0.0005,\n",
    "             learning_rate=\"constant\", # or 'adaptive' etc.\n",
    "             random_state=42)\n",
    "```\n",
    "\n",
    "but as usual, there is a bewildering array of possibilities...we will tackle this problem later when searching for the optimal hyperparameters.\n",
    "\n",
    "NOTE: the `learning_schedule()` method could also have been used in the normal SG algorithm; is not directly part of the stochastic method, but a concept in itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Qc...in text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qd Mini-batch Gradient Descent Method \n",
    "\n",
    "Finally explain what a __mini-batch__ SG method is, and how it differs from the two others.\n",
    "\n",
    "Again, take a peek into the demo code below, to extract the algorithm details...and explain the __main differences__, compared with the GD and SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Qd...run this code\n",
    "\n",
    "# NOTE: code from [GITHOML], 04_training_linear_models.ipynb\n",
    "\n",
    "theta_path_mgd = []\n",
    "\n",
    "n_iterations = 50\n",
    "minibatch_size = 20\n",
    "\n",
    "np.random.seed(42)\n",
    "theta = np.random.randn(2,1)  # random initialization\n",
    "\n",
    "t0, t1 = 200, 1000\n",
    "def learning_schedule(t):\n",
    "    return t0 / (t + t1)\n",
    "\n",
    "t = 0\n",
    "for epoch in range(n_iterations):\n",
    "    shuffled_indices = np.random.permutation(m)\n",
    "    X_b_shuffled = X_b[shuffled_indices]\n",
    "    y_shuffled = y[shuffled_indices]\n",
    "    for i in range(0, m, minibatch_size):\n",
    "        t += 1\n",
    "        xi = X_b_shuffled[i:i+minibatch_size]\n",
    "        yi = y_shuffled[i:i+minibatch_size]\n",
    "        \n",
    "        gradients = 2/minibatch_size * xi.T.dot(xi.dot(theta) - yi)\n",
    "        eta = learning_schedule(t)\n",
    "        theta = theta - eta * gradients\n",
    "        theta_path_mgd.append(theta)\n",
    "\n",
    "print(f'mini-batch theta={theta.ravel()}')\n",
    "print('OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qe Choosing a Gradient Descent Method\n",
    "\n",
    "Explain the $ฮธ_0โฮธ_1$ plot below, and make a comment on when to use GD/SGD/mini-batch gradient descent (pros and cons for the different methods)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Qd...run this code\n",
    "\n",
    "# NOTE: code from [GITHOML], 04_training_linear_models.ipynb\n",
    "\n",
    "theta_path_bgd = np.array(theta_path_bgd)\n",
    "theta_path_sgd = np.array(theta_path_sgd)\n",
    "theta_path_mgd = np.array(theta_path_mgd)\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(theta_path_sgd[:, 0], theta_path_sgd[:, 1], \"r-s\", linewidth=1, label=\"Stochastic\")\n",
    "plt.plot(theta_path_mgd[:, 0], theta_path_mgd[:, 1], \"g-+\", linewidth=2, label=\"Mini-batch\")\n",
    "plt.plot(theta_path_bgd[:, 0], theta_path_bgd[:, 1], \"b-o\", linewidth=3, label=\"Batch\")\n",
    "plt.legend(loc=\"upper left\", fontsize=16)\n",
    "plt.xlabel(r\"$\\theta_0$\", fontsize=20)\n",
    "plt.ylabel(r\"$\\theta_1$   \", fontsize=20, rotation=0)\n",
    "plt.axis([2.5, 4.5, 2.3, 3.9])\n",
    "plt.show()\n",
    "print('OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [OPTIONAL]  Qf Extend the MyRegressor \n",
    "\n",
    "NOTE: this excercise only possible if `linear_regression_2.ipynb` has been solved.\n",
    "\n",
    "Can you extend the `MyRegressor` class from the previous `linear_regression_2.ipynb` notebook, adding a numerical train method? Choose one of the gradient descent methods above...perhaps starting with a plain SG method.\n",
    "\n",
    "You could add a parameter for the class, indicating it what mode it should be operating: analytical closed-form or numerical, like\n",
    "\n",
    "```python  \n",
    "class MyRegressor(BaseEstimator):\n",
    "    def __init__(self, numerical = False):\n",
    "        self.__w = None\n",
    "        self.__numerical_mode = numerical\n",
    "    .\n",
    "    .\n",
    "    .\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Qf...[OPTIONAL]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REVISIONS||\n",
    ":- | :- |\n",
    "2018-02-14| CEF, initial.\n",
    "2018-02-14| CEF, added optional exe.\n",
    "2018-02-20| CEF, major update.\n",
    "2018-02-20| CEF, fixed revision table malformatting.\n",
    "2018-02-25| CEF, removed =0 in expression.\n",
    "2018-02-25| CEF, minor text updates and made Qc optional.\n",
    "2018-02-25| CEF, minor source code cleanups.\n",
    "2021-09-18| CEF, update to ITMAL E21.\n",
    "2021-10-02| CEF, corrected link to extra material and page numbers for HOML v2 (114/115=>121/122).\n",
    "2022-01-25| CEF, update to SWMAL F22.\n",
    "2023-02-22| CEF, updated page no to HOML 3rd. ed., updated to SWMAL F23\n",
    "2023-09-19| CEF, changed LaTeX mbox and newcommand (VSCode error) to textrm/mathrm and renewcommand.\n",
    "2023-09-29| CEF, changed new/renewcommands to defs for full KaTeX/MathJax interoperability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
