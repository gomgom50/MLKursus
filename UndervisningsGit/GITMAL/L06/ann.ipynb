{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWMAL Exercise\n",
    "\n",
    "\n",
    "##  Artificial Neural Networks as Universal Approximators\n",
    "\n",
    "An ANN can in principle approximate any n-dimensional function: given enough neurons (and layers) a ANN is an _universal approximator_.\n",
    "\n",
    "Let us test this by using a very simple ANN consisting of only two neurons in a hidden layer(and an input- and output-layer both with the identity activation function, _I_ ).\n",
    "\n",
    "Given a `tanh` activation function in a neuron, it can only approximate something similar to this monotonic function, but applying two neurons in a pair, they should be able to approximate an up-hill-then-downhill non-monotonic function, which is a simple function with a single maximum. \n",
    "\n",
    "We use Scikit-learns `MLPRegressor` for this part of the exercise. Use the synthetic data, generated by the `GenerateSimpleData()` functions, in the next cells and train the MLP to make it fit the curve. \n",
    "\n",
    "Notice the lack of a train-test split in the exercise; since we only want to look at the approximation capabilities of the MLP, the train-test split is omitted, (and you are welcome to do the split yourself, and also to add noise in the data generators.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One data generator just to test out the MLP..\n",
    "#   An MLP with just two neurons should be able to approximate this simple\n",
    "#   down-up graph using its two non-linear sigmoid or tanh neurons...\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def GenerateSimpleData():\n",
    "    X = np.linspace(-10, 10, 100)\n",
    "    y = 2*np.tanh(2*X - 12) - 3*np.tanh(2*X - 4)  \n",
    "    y = 2*np.tanh(2*X + 2)  - 3*np.tanh(2*X - 4)   \n",
    "    X = X.reshape(-1, 1) # Scikit-algorithms needs matrix in (:,1)-format\n",
    "    return X,y\n",
    "\n",
    "X, y_true = GenerateSimpleData()\n",
    "plt.plot(X, y_true, \"r-.\")\n",
    "plt.legend([\"y_true\"])\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"ANN, Groundtruth data simple\")\n",
    "           \n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qa)\n",
    "\n",
    "Fit the model using the data generator and the MLP in the next cell. \n",
    "\n",
    "Then plot `y_true` and `y_pred` in a graph, and extract the network weights and bias coefficients (remember the `coefs_` and `intercepts_` attributes you found on a linear regressor in an earlier exercise, the MLP is similar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MLP and fit model, just run..\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "mlp = MLPRegressor(activation = 'tanh',      # activation function \n",
    "                   hidden_layer_sizes = [2], # layes and neurons in layers: one hidden layer with two neurons\n",
    "                   alpha = 1e-5,             # regularization parameter\n",
    "                   solver = 'lbfgs',         # quasi-Newton solver\n",
    "                   max_iter=10000,\n",
    "                   verbose = True)\n",
    "\n",
    "mlp.fit(X, y_true)\n",
    "y_pred = mlp.predict(X)\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: plot the fit.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qb)\n",
    "\n",
    "Draw the ANN with its input-, hidden- and output-layer. Remember the bias input to the input- and hidden-layer (a handmade drawing is fine).\n",
    "\n",
    "Now, add the seven weights extracted from the MLP attributes to the drawing: four w coefficients and three bias coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: extract and print all coefficients.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qc)\n",
    "\n",
    "Create a mathematical formula for the network ala\n",
    "\n",
    "    y_math = 0.3* tanh(2 * x + 0.1) - 0.3 * tanh(5 * x + 3) + 0.9\n",
    "\n",
    "with the seven weights found before, two or three decimals should be sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: create formula.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qd)\n",
    "\n",
    "Plot the `y_math` function using `np.tanh` and `X` as input similar to  \n",
    "\n",
    "    y_math = 0.3*np.tanh(2 * X + ..\n",
    "   \n",
    "and compare `y_math` with `y_pred` and `y_true` in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: plot the formula.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qe)\n",
    "\n",
    "Plot the first half of the function ala\n",
    "\n",
    "    y_math_first_part = 0.3* tanh(2 * X + 0.1)\n",
    "   \n",
    "and then plot the second part. The sum of these two parts gives the total value of y_math if you also add them with the last bias part.\n",
    "\n",
    "Are the first and second parts similar to a monotonic tanh activation function, and explain the ability of the two-neuron network to be a general approximator for the input function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: plot the first and second half of the formula.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qf)\n",
    "\n",
    "Now we change the data generator to a `sinc`-like function, which is a function that needs a NN with a higher capacity than the previous simple data.\n",
    "\n",
    "Extend the MLP with more neurons and more layers, and plot the result. Can you create a good approximation for the `sinc` function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateSincData():\n",
    "    # A Sinc curve, approximation needs more neurons to capture the 'ringing'...\n",
    "    X = np.linspace(-3, 3, 1000) \n",
    "    y = np.sinc(X)\n",
    "    X = X.reshape(-1,1)\n",
    "    return X, y\n",
    "\n",
    "X, y_true = GenerateSincData()\n",
    "plt.plot(X, y_true, \"r-\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"ANN, Groundtruth data for Sinc\")\n",
    "\n",
    "# TODO:\n",
    "assert False, \"TODO: instantiate and train an MLP on the sinc data..\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  [OPTIONAL] Qg) \n",
    "\n",
    "Change the hyperparameters in the MLP, say the `alpha` to `1e5` and `1e-1`, and explain the results (hint: regularization).\n",
    "\n",
    "Also, try out different `activation` functions `learning_rate`s and `solver`s, or other interesting hyperparameters found on the MLP regressor in the documentation.\n",
    "\n",
    "Finally, implement the MLP regressor in `Keras` instead.\n",
    "\n",
    "(Solvers aka. optimizers and regularization will be discussed in a later lecture.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: do some experiments.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REVISIONS||\n",
    ":- | :- |\n",
    "2021-10-04| CEF, initial, converted from old word format.\n",
    "2021-10-04| CEF, inserted ANN_example.py into Notebook.\n",
    "2023-03-06| CEF, minor table update.\n",
    "2023-03-09| CEF, major update, translated to English, elaborated on NNs as Universal Approximator."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
